from transformers import VisionEncoderDecoderModel, AutoTokenizer, ViTImageProcessor
from PIL import Image
import requests
from io import BytesIO

# Path to the directory containing the model and tokenizer
model_path = 'C:\\User\\ppra\\.cache\\huggingface\\hub\\models--nlpconnect--vit-gpt2-image-captioning\\snapshots\\dc68f91c06a1ba6f15268e5b9c13ae7a7c514084'

# Load the model and tokenizer
model = VisionEncoderDecoderModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path, clean_up_tokenization_spaces=True)
processor = ViTImageProcessor.from_pretrained(model_path)

def load_image(image_path):
    if image_path.startswith('http'):
        response = requests.get(image_path)
        image = Image.open(BytesIO(response.content)).convert('RGB')
    else:
        image = Image.open(image_path).convert('RGB')
    return image

def generate_caption(image_path):
    image = load_image(image_path)
    
    # Display the image
    image.show()
    
    inputs = processor(images=image, return_tensors="pt")
    pixel_values = inputs['pixel_values']

    # Generate caption
    output_ids = model.generate(pixel_values, max_length=50, num_beams=5)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

# Example usage
image_path = 'image 1.jpeg'  # Replace with your image URL or local path
caption = generate_caption(image_path)
print(f"Caption: {caption}")
